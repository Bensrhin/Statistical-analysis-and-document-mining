---
title: "SADM_Lab3"
author: "El Ansary Otmane, Benchekroun Omar, Bensrhier Nabil"
date: "15/04/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




## Abstract

Each team has to upload a report on Teide before April 30 at 20:00. All results should be commented and interpreted. Answer to the questions in order and refer to the question number in your report. Computations and graphics have to be performed with R. The report should be written using the Rmarkdown format. From your .rmd file, you are asked to generate an .html file and upload them both on Teide. In the .html file, you should limit the displayed R code to the most important instructions.






In this tutorial, we consider a dataset Pima Indians Diabetes from the mlbench package. This data set contains descriptions of patients that are females at least 21 years old of Pima Indian heritage. The goal is to predict whether a patient has signs of diabetes. The data is supervised i.e. the labels are known.

```{r}
library(mlbench)
data(PimaIndiansDiabetes2)
set.seed(0)

```


## Question 1:

### 1) - What type of features we have (numerical, categorical, rank)? How many missing values we have and which features contain them?


- numerical : pregnant - glucose - pressure - triceps - insulin - mass - pedigree - age
- categorical : diabetes
- rank : age


The following features have missing values : 
```{r echo=FALSE}
total <- 0
missing <- c()
number <- c()
for (i in 1 : dim(PimaIndiansDiabetes2)[2]){
  # the number of missing values in each column
  nb_missing <- sum(is.na(PimaIndiansDiabetes2[,i]))
  if (nb_missing > 0){
    # feature that has missing values
    feature <- names(PimaIndiansDiabetes2)[i]
    missing <- c(missing, feature)
    #cat(feature)
    #cat(" : ")
    # the number of missing values in that feature
    number <- c(number, nb_missing)
    #cat(number)
    #cat("\n")
    total = total  + nb_missing
  }
}
# Visualization of results
ylim <- c(0, 1.1*max(number))
ax <- barplot(number,beside=T,col=c("#F7D358"), names=missing, ylim=ylim,
              main = "Features that have missing values",
              ylab="number of missing values")
text(x=ax, y<-number, label=number, pos = 3, cex = 0.8, col = "red")

```


```{r echo=FALSE}
cat("With a total missing values of :", total)
```


### 2) - How many positive and negative examples we have? Is the data set balanced?


```{r echo=FALSE}
positive <- sum(PimaIndiansDiabetes2$diabetes=='pos')
cat("We have :", positive, "positive examples.")
```
And 
```{r echo=FALSE}
# Two possible methods:
negative <- sum(PimaIndiansDiabetes2$diabetes=='neg')
negative <- dim(PimaIndiansDiabetes2)[1] - positive
cat("We have :", negative, "negative examples.")
```
```{r echo=FALSE}
# Visualization of results
y <- c(positive, negative)
names <- c("postive", "negative")
col <- c("green", "red")
ylim <- c(0, 1.1*max(y))
ax <- barplot(y,beside=T,col=col, names=names, ylim=ylim,
              main = "positive and negative examples in the dataset")
text(x=ax, y<-y, label=y, pos = 3, cex = 0.8, col = col)
```



Since there are twice as many negative examples as the positive ones, there is a high difference between these two classes. Therefore, our dataset is imbalanced. 



## Question 2:

Using the following code, import the file aux_functions.r and convert the labels into the integer format:
```{r}
source('aux_functions.r')
x = PimaIndiansDiabetes2[,-9]
y = PimaIndiansDiabetes2[,9]
y = negative_positive_class_labels(y)
```


### 1) - Write a function that returns the accuracy score:
```{r}
accuracy_score <- function(y_true, y_pred){
  if (length(y_true)==length(y_pred))
  {
    return(sum(y_pred==y_true)/length(y_true))
  }
  else
  {
    stop("ERROR : vectors must be of equal length")  
  }
}
```


### 2) - What is the accuracy score of a dummy classifier that predicts the negative class only?

A dummy classifier uses some simple computation like frequency of majority class instead of fitting.
```{r}
 
y_pred = rep(-1, length(y))
dummy_acc <- accuracy_score(y, y_pred)
```

It takes the majority count of the imbalanced class which is the percentage of the negative examples.
```{r echo=FALSE}
cat("The accuracy of a dummy classifier is :", dummy_acc)
```

## Question 3:

In this question, we will study basic tricks to deal with missing values.

### 1) - Firstly, replace the NAs by 0. Then, classify the data using the LDA from the MASS library in the leave-one-out fashion (see the documentation of lda) and report the performance.
```{r}
# replace NAs by 0
x0 <- x
x[is.na(x)] = rep(0, sum(is.na(x)))
```

```{r}
# replace NAs by 0
library(MASS)
model <- lda(y~., data=x, CV=TRUE)
acc_0 <- accuracy_score(model$class, y)
```
```{r echo=FALSE}
x_best <- x
y_best <- y
cat("The accuracy of this model is :", acc_0)
```


### 2) - Now, for each feature, replace the NAs by its mean value. Report the performance.

```{r}
x <- x0
for (i in 1:dim(x)[2]){
  if (sum(is.na(x[,i]))>0){
    f = x[,i]
    m = mean(f[!is.na(f)])
    f[is.na(f)] = rep(m, sum(is.na(f)))
    x[,i] = f
  }
}

model <- lda(y~., data=x, CV=TRUE)
acc_mean <- accuracy_score(model$class, y)
cat("The accuracy of this model is :", acc_mean)
```


### 3) - Finally, remove observations where the missing values appear. Report the performance and make the conclusion which imputation works better on this data set. Try to explain why this could happen. Save the imputed data with the best performance for further questions.

```{r}
x = x0
bool = NULL
for (i in 1:dim(x)[1]){
  bool = c(bool, sum(is.na(x[i,]))==0) 
}

model <- lda(y[bool]~., data=x[bool,], CV=TRUE)
acc_no_na <- accuracy_score(model$class, y[bool])
cat("The accuracy of this model is :", acc_no_na)
```
```{r}
# Visualization of results
accuracy <- c(acc_0, acc_mean, acc_no_na)
names <- c("NA -> 0", "NA -> mean", "removing NA")
#ylim <- c(0, 1.5*max(accuracy))
#ax <- barplot(accuracy,beside=T,col=c("red", "red", "green"), names=names, ylim=ylim,
#              main = "Accuracy of the three models",
#              ylab="Accuracy")
#text(x=ax, y<-accuracy, label=accuracy, pos = 3, cex = 0.8, col = "blue")
data.frame(names, accuracy)
```

**Results: **
The model with the best performance is the one where we delete NA's, the amount of data amputated is very considerable (~50%), so we may have to change this decision later on if needed.
We save the imputed data with the best performance :

```{r}
x <- x[bool,]
y <- y[bool]
```


## Question 4:

In this question, we compare various classification models by computing 10-fold cross-validation score.

### 1) - Create a partition of the data set by 10 folds. Use the function createFolds from the library caret. Read the documentation to choose a convenient way to output folds.

```{r}
library(caret)
folds = createFolds(y, 10)
y_split = lapply(folds, function(ind, dat) dat[ind], dat = y)
x_split <- lapply(folds, function(ind, dat) dat[ind,], dat = x)
```


### 2) - Perform the 10-fold cross-validation, where the following models are compared: LDA, kNN (k=1), kNN (k=5), kNN (k=10), perceptron (eta=0.1, num_iter=1000). Then, report mean and standard deviation across 10 folds for each method. Comment observed results. The function knn can be taken from the library class. The perceptron function is available at aux_functions.r with the corresponding predict.perceptron function.

```{r}
library(class)

CV <- function(my_model){
  accs = NULL
  for (i in 1:10){
    x_test = x_split[i][[1]]
    y_test = y_split[i][[1]]
    x_train = NULL
    y_train = NULL
    for (el in x_split[-i]){
      x_train = rbind(x_train, el)
    }
    for (el in y_split[-i]){
      y_train = c(y_train, el)
    }
    y_pred = my_model(x_train, y_train, x_test)
    accs = c(accs, accuracy_score(y_pred, y_test))
  }
  return (accs)
}

model_lda <- function(x_train, y_train, x_test){
   model <- lda(y_train~., data=x_train)
   y_pred = predict(model, newdata=x_test)$class
   return(y_pred)
}

model_knn1 <- function(x_train, y_train, x_test) {
  return (knn(x_train, x_test, y_train, k=1, prob=TRUE))
}
model_knn5 <- function(x_train, y_train, x_test) {
  return (knn(x_train, x_test, y_train, k=5, prob=TRUE))
}
model_knn10 <- function(x_train, y_train, x_test) {
  return (knn(x_train, x_test, y_train, k=10, prob=TRUE))
}

model_perceptron <- function(x_train, y_train, x_test) {
  per = perceptron(x_train, y_train)
  return (predict.perceptron(per, x_test))
}

models = c(model_lda, model_knn1, model_knn5, model_knn10, model_perceptron)
res = sapply(models, function(x) return(CV(x)))

noms = c("lda", "knn1", "knn5", "knn10", "perceptron")
accs = sapply(1:5, function(x) return(mean(res[,x])))
sds = sapply(1:5, function(x) return(sd(res[,x])))
data.frame(noms, accs, sds)
```

The model with the best accuracy is LDA, and is almost as performant as knn with k=10. The lowest performance reported is that of the perceptron, we can also notice that the value of the mean of the accuracies changes too much with every execution, which probably means its not appropriate for our case.



### 3) - What statistical test we can use to verify whether the classifier with the highest accuracy is significantly better than the others on some level of significance?


Since we have a small sample size (=10), a t-test is more appropriate (than a z-test for example).
We are going to perform 4 t-tests, comparing the mean of the best model (lda) with the rest, the computed p-values are :

```{r}
pvalues <- sapply(2:5, function(i) return(t.test(res[,1], res[,i], alternative = "two.sided", var.equal = FALSE)$p.value))
pvalues
data.frame(noms[2:5],pvalues)
```

The lowest p-value corresponds to lda VS perceptron, which is predictable since the difference of the mean of the accuracies is the biggest and the perceptron model has a very high standard deviation.
We can conclude that lda is definitely better than perceptron, but the difference between the others (knn) has little statistical significance, especially knn for k=10.



## Question 5:

In this question, we will dive deeper to see whether classification results are adequate. For sake of simplicity, we will limit computations on one train/test by taking the 1st fold as the test test, whereas the rest folds for the train set.

```{r}
x_test = x_split[1][[1]]
y_test = y_split[1][[1]]
x_train = NULL
y_train = NULL
for (el in x_split[-1]){
  x_train = rbind(x_train, el)
}
for (el in y_split[-1]){
  y_train = c(y_train, el)
}
```

### 1) - Evaluate the confusion matrix for the classifier with the best accuracy score. Comment observed results and make a conclusion whether these results are appropriate for applications like diabet prediction.

```{r}
model <- lda(y_train~., data=x_train)
y_pred = predict(model, newdata=x_test)$class
table(predicted = y_pred, observed = y_test)
```

There are more false negatives than false positives, which is not appropriate for most medical applications, since telling a patient that he's not sick when he is, is more life threatning than a 'false alarm'. Same thing applies for the novel Covid19 tests for example.
It is therefore necessary to distinguish between the two errors.


### 2) - Write a function that returns the sensitivity score a.k.a. the true positive rate. Would the sensitivity score be a more appropriate metric in our case?

```{r}
sensitivity_score <- function(y_true, y_pred){
  TP <- y_true[y_true == y_pred]
  TP <- sum(TP[TP==1])
  FN <- y_true[y_true != y_pred]
  FN <- -sum(FN[FN==-1])
  return (TP/(TP+FN))
}
```

Sensitivity refers to the test's ability to correctly detect ill patients who do have the condition, it is therefore a more appropriate metric in this case.


### 3) - Compute the sensitivity score for each classification approach under consideration. Comment the observed results.

```{r}
predict_sen <- function(model){
  y_pred =as.numeric(as.character( model(x_train, y_train, x_test)))
  return (sensitivity_score(y_pred, y_test))
}
res = sapply(models, function(x) return(predict_sen(x))); 
data.frame(models = noms, sensitivity=res)
```

The model with the best performance is
```{r}
res = res[1:4]
noms[1:4][res==max(res)]
```
We ignored the value computed by the perceptron because it changes drastically in every execution (from 0 to 1).

Surprisingly, knn performance descreases with k, which was not the case when we had the accuracy as a metric. 

Overall, the computed sensitivity varies too much with every execution, which is due to the fact that we did not perform cross validation this time.


## Bonus Question:

Propose and implement one/several solutions to improve the performance of the perceptron.
