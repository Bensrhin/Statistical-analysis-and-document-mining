---
title: "SADM_Lab1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Question 1:

```{r}
set.seed(0)
n = 6000; m = 201
vect = rnorm(n*m)
m = matrix(vect,nrow = n, ncol = m)
d = as.data.frame(m)
```


Question 2:

Multiple linear regression model:
Using the 200 predictors,
$$
\forall i \in [1,6000] \space x_{0 i} = \sum_{j=1}^{200}\beta_{j} x_{ji} + \beta_0 + \epsilon_i
$$

True regression model: (?? A VERIFIER)
$$
 \space x_{0} = \sum_{x=1}^{1206000} \beta_{i} x_{i} + \beta_0 + \epsilon_0
$$

Question 3 :

```{r}
reg<-lm(d$V1 ~ ., d)
summary(reg)$coefficients
```


```{r}
summary(reg)$coefficients[summary(reg)$coefficients[,4]<=0.05,] #?? A VERIFIER
```



Question 4:

```{r}
n = 1000
eps1 = rnorm(n)
eps2 = rnorm(n)
eps3 = rnorm(n)
x1 = eps1
x2 = 3*x1 + eps2
y = x1 + x2 + 2 + eps3
plot(x1,x2)
```

$X_2$ and $X_1$ are clearly correlated. The plot resembles that of the function $y=3x$ with a fluctuation around it due to $\epsilon_2$.




Question 5:

```{r}
reg1 <- lm(y~x1)
summary(reg1)$coefficients
beta1 = summary(reg1)$coefficients[2,1]; beta1
beta0 = summary(reg1)$coefficients[1,1]; beta0
sigmat1 = summary(reg1)$coefficients[1,2]; sigmat1 #?? (A VERIFIER)

reg2 <- lm(y~x2)
summary(reg2)$coefficients
beta2 = summary(reg2)$coefficients[2,1]; beta2
sigmat2 = summary(reg2)$coefficients[1,2]; sigmat2
```

The coefficients computed are quite close to the real values, except maybe for $\beta_2$, because of the presence of two random variables in the simulated values.


```{r}
set.seed(3)
n = 10
eps1 = rnorm(n)
eps2 = rnorm(n)
eps3 = rnorm(n)
x1 = eps1
x2 = 3*x1 + eps2
y = x1 + x2 + 2 + eps3
reg1 <- lm(y~x1)
beta1 = summary(reg1)$coefficients[2,1]; beta1
beta0 = summary(reg1)$coefficients[1,1]; beta0
sigmat1 = summary(reg1)$coefficients[1,2]; sigmat1

reg2 <- lm(y~x2)
beta2 = summary(reg2)$coefficients[2,1]; beta2
sigmat2 = summary(reg2)$coefficients[1,2]; sigmat2
```

By lowering the value of $n$, the computed values are less accurate, the variances $\tilde{\epsilon}_j$ are also bigger which indicates that the precision of the regression has decreased significantly.


```{r}
reg3 = lm(y~x1+x2)
#summary(reg3)
beta2 = summary(reg3)$coefficients[3,1]; beta2
beta1 = summary(reg3)$coefficients[2,1]; beta1
beta0 = summary(reg3)$coefficients[1,1]; beta0
sigma = summary(reg3)$coefficients[1,2]; sigma
```

The values are once again accurate. Because ?? (TO FILL WITH SOME BS)








PART 2 :


1 - 
```{r}
prostateCancer<-read.table("./prostate.data",header=T);
attach(prostateCancer)
```

```{r}
pro <- prostateCancer[1:9]
pairs(pro)
```

There is an apparently strong correlation between lcavol and : svi, lcp and lpsa.




2 -

(a)
```{r}
pro$gleason<-factor(pro$gleason)
pro$svi<-factor(pro$svi)

reg <- lm(pro$lcavol ~., pro)
summary(reg)
```


-  We notice the presence of gleason7, 8 and 9 instead of gleason. Each one of these variables represents a level of the variable we made quantitative (using the command factor()). Basically, instead of performing a regression based on the 4 possible values of gleason, we seperate the variable gleason into 3 variables, each one representing a simple "YES" or "NO" if the variable is equal to the corresponding level (we don't need a fourth one, since its value can be deduced from the 3 others). 
The same thing applies for the variable svi, with 2 levels, we find ourselves with one variable in the linear regression, with the same meaning as described before.


-  There is a strong correlation between lcavol and : lcp and lpsa with a p-value inferior to $10^{-5}$, and a weak correlation between lcavol and pgg45 with a p-value inferior to $0.1$.




(b) 
```{r}
confint(reg)
```

COMMENT MISSING



(c)
```{r}
reg_lpsa <- lm(pro$lcavol ~lpsa)
summary(reg_lpsa)$coefficients
confint(reg_lpsa)
```

lpsa is a very strong predictor of lcavol.


(d)

```{r}
plot(pro$lcavol, reg$fitted.values)
```



```{r}
hist(reg$residuals, prob=TRUE)
norm <- function(x){
  return (dnorm(x, sd = sd(reg$residuals)))
}
par(new=TRUE)
curve(norm(x), add=TRUE)
```

Yes, we can admit that the residuals are normally distributed.

The residual sum of squares :
```{r}
sum(reg$residuals^2)
```


(e)
dunno, looks good

(f)
```{r}
reg_f = lm(pro$lcavol ~ pro$lweight + pro$age + pro$lbph + pro$svi + pro$gleason + pro$pgg45)
summary(reg_f)
```

By plotting the fitted values against the real values :
```{r}
plot(pro$lcavol, reg_f$fitted.values)
```

By removing the best predictors, the model becomes quite inaccurate in comparison to the previous one, even if the p-value of svi1 is very low, that's because that variable is the best predictor between all the predictors available, and is not an indicator of the validity of the model in itself.



3 -  

```{r}
aov_svi<-aov(pro$lcavol~pro$svi, pro); summary(aov_svi)
aov_gleason<-aov(pro$lcavol~pro$gleason, pro); summary(aov_gleason)
avo2 <-aov(pro$lcavol~pro$gleason*pro$svi, pro); summary(avo2)
```

By performing the one way ANOVA on the two qualitative variables, we can deduce that they're statistically very significant to predict lcavol.
The two way ANOVA tells us that the interaction between the two variables, however, is not statistically significant at all (p-value too high).





4 - 

(a)
```{r}
reg1<-lm(lcavol ~ 1, data=pro)
reg2<-lm(lcavol ~ .,data=pro[,c(1,4,9)])
reg3<-lm(lcavol ~ ., data=pro[,c(1,2,9)])
sum(reg1$residuals^2)
sum(reg2$residuals^2)
sum(reg3$residuals^2)
```


(b)
```{r}
res_min = 1000
cb = combn(9,2)
for (i in 1:dim(cb)[2]){
  c<-cb[,i]
  reg<-lm(lcavol ~ .,data=pro[,c])
  if(sum(reg$residuals^2)<res_min){
    res_min = sum(reg$residuals^2)
    min = c
  }
}
cat(res_min); cat("\n"); cat(names(pro)[min]); cat("\n")
```


The best couple of predictors is (lcp, lpsa), just as deduced in former questions.



(c)

```{r}
ress = c()

#k=0
reg<-lm(lcavol ~ 1, data=pro);
ress = c(ress, sum(reg$residuals^2))
cat("k = 0", "\n"); 
cat(sum(reg$residuals^2)); cat("\n");
cat("--\n\n")

#k=1
for (k in 2:9){
  res_min = 1000
  reg<-lm(lcavol ~ pro[,k])
  if(sum(reg$residuals^2)<res_min){
    res_min = sum(reg$residuals^2)
    min = k
  }
}
ress = c(ress, sum(reg$residuals^2))
cat("k = 1", "\n"); 
cat(res_min); cat("\n");
cat(names(pro)[min]); cat("\n\n")

#k from 2 to 8
for(k in 2:8){
  res_min = 1000
  cb = combn(9,k)
  for (i in 1:dim(cb)[2]){
    c<-cb[,i]
    reg<-lm(lcavol ~ .,data=pro[,c])
    if(sum(reg$residuals^2)<res_min){
      res_min = sum(reg$residuals^2)
      min = c
    }
  }
  ress = c(ress, sum(reg$residuals^2))
  cat("k = ", k, "\n"); 
  cat(res_min); cat("\n");
  cat(names(pro)[min]); cat("\n\n")
}
```

N.B : the set of best $k$ predictors is not necessarily a subset of the set of the best $k'$ predictors, such that $k<k'$. The example $k=4$ and $k'=5$ illustrates this point.

```{r}
plot(ress)
par(new=TRUE)
lines(ress)
```

The residual sum of squares is decreasing, which is completely predictable : the more predictors we have, the more we know about the data and therefore the fitted values get closer to the real values.




(d)
dunno..


5 - 

A reasonable model would be to predict lcavol basek on 6 predictors : age lbph lcp gleason pgg45 lpsa. Further predictors can be added to decrease the residual sum of squares, but not considerably.




