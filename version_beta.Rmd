---
title: "SADM_Lab1"
author: "El Ansary Otmane, Benchekroun Omar, Bensrhier Nabil"
date: "01/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Question 1:

```{r}
set.seed(0)
n = 6000; m = 201
vect = rnorm(n*m)
m = matrix(vect,nrow = n, ncol = m)
d = as.data.frame(m)

# méthode 2:
matrice = matrix(rnorm(201 * 6000, mean = 0, sd = 1), 201, 6000)
dataFrame = as.data.frame(t(matrice))
# Each of these columns is referred to as a “variable”
for (i in 1:201)
{
  colnames(dataFrame)[i] <- paste("variable", i, sep="")
}
```


#Question 2:

**Multiple linear regression model:**
Using the 200 predictors,
$$
\forall i \in [1,6000] \space x_{0 i} = \sum_{j=1}^{200}\beta_{j} x_{ji} + \beta_0 + \epsilon_i
$$

True regression model: (?? A VERIFIER)

Je pense the true model signifie le fait de determiner les valeurs des betas
dans ce cas
$$
 \space x_{0} = \sum_{x=1}^{1206000} \beta_{i} x_{i} + \beta_0 + \epsilon_0
$$
Je pense the true model signifie le fait de determiner les valeurs des betas
dans ce cas:
tant que les colonnes sont indépendantes alors il n'y a pas de corrélation entre
la colonnes $X_0$ et les autres colonnes, donc :

$$
\forall i \in [1,6000] \quad \space x_{0 i} =  \epsilon_i \quad and \quad \epsilon_i 
\sim \mathcal{N}(0,\,1)\,.
    
$$
**Comment :**
...

#Question 3 :

```{r}
# faut choisir d[, -1] au lieu de d : car on a besoin seulement des 200 dernières colonnes
reg<-lm(d$V1 ~ ., d)
summary(reg)$coefficients[summary(reg)$coefficients[,4]<=0.05,] #?? A VERIFIER
```


```{r}
# attach(dataFrame) : makes variables available by their names directly.

# we can use variable1 instead of dataFrame$variable1 
reg <- lm(dataFrame$variable1 ~ .,dataFrame[,-1])

# The number of coefficients assessed as significantly non-zero at level 5%
x<-summary(reg)$coefficients
a = (0<x[,4] & x[,4]<=0.05)
cat("The number of coefficients assessed as significantly non-zero at level 5% is :\n", sum(a), "coefficients")
```
**Comment : **
The proportion of these coefficients is equal to $\frac{11}{200} \approx 5.5 \%$ .
As a result, this means that the correlation between columns is almost null.

#Question 4:
```{r}
data <- function(n)
{
  eps1 <<- rnorm(n)
  eps2 <<- rnorm(n)
  eps3 <<- rnorm(n)
  x1 <<- eps1
  x2 <<- 3*x1 + eps2
  y <<- x1 + x2 + 2 + eps3
}

```

```{r}
n = 1000
data(n)
plot(x1, x2, main="Cloud of points", xlab="X1", ylab="X2")
reg<-lm(x2~x1)
lines(x1, fitted.values(reg))
```
**Comment : **
$X_2$ and $X_1$ are clearly correlated. The plot resembles that of the function $y=3x$ with a fluctuation around it due to $\epsilon_2$.


#Question 5:

```{r}
simulate <- function()
{
  # plotting 2 grahs at the same line
  par(mfcol=c(1,2))
  
  # Model 1
  reg1 <- lm(y~x1)

  # coefficients
  beta0 = summary(reg1)$coefficients[1]; 
  beta1 = summary(reg1)$coefficients[2]; 
  sigma2 = summary(reg1)$sigma^2; 
  cat("Estimated coefficients in Model 1:\n")
  cat("beta0 = ", beta0, "\n")
  cat("beta1 = ", beta1, "\n")
  cat("sigma2 = ", sigma2, "\n")
  
  
  # Model 2
  reg2 <- lm(y~x2)

  # coefficients
  beta0 = summary(reg2)$coefficients[1]; 
  beta1 = summary(reg2)$coefficients[2]; 
  sigma2 = summary(reg2)$sigma^2; 
  cat("\nEstimated coefficients in Model 2:\n")
  cat("beta0 = ", beta0, "\n")
  cat("beta1 = ", beta1, "\n")
  cat("sigma2 = ", sigma2, "\n")
  
  # visualizations
  
  plot(x1, y, main="Cloud of points : Model 1", xlab="X1", ylab="Y")
  lines(x1, fitted.values(reg1))
  
  plot(x2, y, main="Cloud of points : Model 2", xlab="X2", ylab="Y")
  lines(x2, fitted.values(reg2))
}
```
Let's write mathematical equations of true models:

Model 1:
$$
Y_i = X_{2,i} + X_{1,i} + 2 + \epsilon_{3,i} \\ 
Y_i = 3X_{1,i} + \epsilon_{2,i} + X_{1,i} + 2 + \epsilon_{3,i} \\
Y_i = 4X_{1,i} + 2 + (\epsilon_{3,i} + \epsilon_{2,i})\\
\boxed{Y_i = 4X_{1,i} + 2 + \epsilon_{4,i}} \quad with \quad 
\epsilon_{4,i} = \epsilon_{3,i} + \epsilon_{2,i} \sim \mathcal{N}(0,\,\sqrt2)\\
$$
Model 2:
$$
Y_i = X_{2,i} + X_{1,i} + 2 + \epsilon_{3,i} \\ 
Y_i = \frac{1}{3}X_{2,i} - \frac{1}{3}\epsilon_{2,i} + X_{1,i} + 2 + \epsilon_{3,i} \\
Y_i = \frac{4}{3}X_{2,i} + 2 + (\epsilon_{3,i} - \frac{4}{3}\epsilon_{2,i})\\
\boxed{Y_i = \frac{4}{3}X_{2,i} + 2 + \epsilon_{5,i}} \quad with \quad 
\epsilon_{5,i} = \epsilon_{3,i} - \frac{1}{3}\epsilon_{2,i} \sim \mathcal{N}(0,\,\sqrt{\frac{10}{9}} \approx 1.054) \\ 
$$
```{r}
simulate()
```

The coefficients computed are quite close to the real values. 

```{r}
set.seed(3)
n = 10
data(n)
simulate()
```


By lowering the value of $n$, the computed values are less accurate, the variances $\tilde{\epsilon}_j$ are also bigger which indicates that the precision of the regression has decreased significantly.

```{r}
# Model 3
reg3 <- lm(y~x1+x2)
# coefficients
beta0 = summary(reg3)$coefficients[1] 
beta1 = summary(reg3)$coefficients[2]
beta2 = summary(reg3)$coefficients[3]
sigma2 = summary(reg3)$sigma^2; 
cat("\nEstimated coefficients in Model 3:\n")
cat("beta0 = ", beta0, "\n")
cat("beta1 = ", beta1, "\n")
cat("beta2 = ", beta2, "\n")
cat("sigma2 = ", sigma2, "\n")
```

The values are once again accurate. 

PART 2 :


1 - 
```{r}
prostateCancer<-read.table("./prostate.data",header=T);
attach(prostateCancer)
```

```{r}
pro <- prostateCancer[1:9]
pairs(pro)
```

There is an apparently strong correlation between lcavol and : svi, lcp and lpsa.




2 -

(a)
```{r}
pro$gleason<-factor(pro$gleason)
pro$svi<-factor(pro$svi)

reg <- lm(pro$lcavol ~., pro)
summary(reg)
```


-  We notice the presence of gleason7, 8 and 9 instead of gleason. Each one of these variables represents a level of the variable we made quantitative (using the command factor()). Basically, instead of performing a regression based on the 4 possible values of gleason, we seperate the variable gleason into 3 variables, each one representing a simple "YES" or "NO" if the variable is equal to the corresponding level (we don't need a fourth one, since its value can be deduced from the 3 others). 
The same thing applies for the variable svi, with 2 levels, we find ourselves with one variable in the linear regression, with the same meaning as described before.


-  There is a strong correlation between lcavol and : lcp and lpsa with a p-value inferior to $10^{-5}$, and a weak correlation between lcavol and pgg45 with a p-value inferior to $0.1$.




(b) 
```{r}
confint(reg)
```

COMMENT MISSING



(c)
```{r}
reg_lpsa <- lm(pro$lcavol ~lpsa)
summary(reg_lpsa)$coefficients
confint(reg_lpsa)
```

lpsa is a very strong predictor of lcavol.


(d)

```{r}
plot(pro$lcavol, reg$fitted.values)
```



```{r}
hist(reg$residuals, prob=TRUE)
norm <- function(x){
  return (dnorm(x, sd = sd(reg$residuals)))
}
par(new=TRUE)
curve(norm(x), add=TRUE)
```

Yes, we can admit that the residuals are normally distributed.

The residual sum of squares :
```{r}
sum(reg$residuals^2)
```


(e)
dunno, looks good

(f)
```{r}
reg_f = lm(pro$lcavol ~ pro$lweight + pro$age + pro$lbph + pro$svi + pro$gleason + pro$pgg45)
summary(reg_f)
```

By plotting the fitted values against the real values :
```{r}
plot(pro$lcavol, reg_f$fitted.values)
```

By removing the best predictors, the model becomes quite inaccurate in comparison to the previous one, even if the p-value of svi1 is very low, that's because that variable is the best predictor between all the predictors available, and is not an indicator of the validity of the model in itself.



3 -  

```{r}
aov_svi<-aov(pro$lcavol~pro$svi, pro); summary(aov_svi)
aov_gleason<-aov(pro$lcavol~pro$gleason, pro); summary(aov_gleason)
avo2 <-aov(pro$lcavol~pro$gleason*pro$svi, pro); summary(avo2)
```

By performing the one way ANOVA on the two qualitative variables, we can deduce that they're statistically very significant to predict lcavol.
The two way ANOVA tells us that the interaction between the two variables, however, is not statistically significant at all (p-value too high).





4 - 

(a)
```{r}
reg1<-lm(lcavol ~ 1, data=pro)
reg2<-lm(lcavol ~ .,data=pro[,c(1,4,9)])
reg3<-lm(lcavol ~ ., data=pro[,c(1,2,9)])
sum(reg1$residuals^2)
sum(reg2$residuals^2)
sum(reg3$residuals^2)
```


(b)
```{r}
res_min = 1000
cb = combn(9,2)
for (i in 1:dim(cb)[2]){
  c<-cb[,i]
  reg<-lm(lcavol ~ .,data=pro[,c])
  if(sum(reg$residuals^2)<res_min){
    res_min = sum(reg$residuals^2)
    min = c
  }
}
cat(res_min); cat("\n"); cat(names(pro)[min]); cat("\n")
```


The best couple of predictors is (lcp, lpsa), just as deduced in former questions.



(c)

```{r}
ress = c()

#k=0
reg<-lm(lcavol ~ 1, data=pro);
ress = c(ress, sum(reg$residuals^2))
cat("k = 0", "\n"); 
cat(sum(reg$residuals^2)); cat("\n");
cat("--\n\n")

#k=1
for (k in 2:9){
  res_min = 1000
  reg<-lm(lcavol ~ pro[,k])
  if(sum(reg$residuals^2)<res_min){
    res_min = sum(reg$residuals^2)
    min = k
  }
}
ress = c(ress, sum(reg$residuals^2))
cat("k = 1", "\n"); 
cat(res_min); cat("\n");
cat(names(pro)[min]); cat("\n\n")

#k from 2 to 8
for(k in 2:8){
  res_min = 1000
  cb = combn(9,k)
  for (i in 1:dim(cb)[2]){
    c<-cb[,i]
    reg<-lm(lcavol ~ .,data=pro[,c])
    if(sum(reg$residuals^2)<res_min){
      res_min = sum(reg$residuals^2)
      min = c
    }
  }
  ress = c(ress, sum(reg$residuals^2))
  cat("k = ", k, "\n"); 
  cat(res_min); cat("\n");
  cat(names(pro)[min]); cat("\n\n")
}
```

N.B : the set of best $k$ predictors is not necessarily a subset of the set of the best $k'$ predictors, such that $k<k'$. The example $k=4$ and $k'=5$ illustrates this point.

```{r}
plot(ress)
par(new=TRUE)
lines(ress)
```

The residual sum of squares is decreasing, which is completely predictable : the more predictors we have, the more we know about the data and therefore the fitted values get closer to the real values.




(d)
dunno..


5 - 

A reasonable model would be to predict lcavol basek on 6 predictors : age lbph lcp gleason pgg45 lpsa. Further predictors can be added to decrease the residual sum of squares, but not considerably.




