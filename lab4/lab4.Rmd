---
title: "lab4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
word_count_train <- read.csv("datasets/word_count_train.csv")
word_count_test <- read.csv("datasets/word_count_test.csv")
tfidf_train <- read.csv("datasets/tfidf_train.csv")
tfidf_test <- read.csv("datasets/tfidf_test.csv")
```

# Question 1:
disadvantages of dense matrices (compared to sparse matrices TODO)


# Question 2:

## What is the vocabulary size? 

(TODO)

## For the review No.7 :

### The most frequent word is :

```{r}
col <- dim(word_count_train)[2]
No.7 <- t(word_count_train[7,6:col])
names(word_count_train[,6:col])[(which(No.7 == max(No.7)))]
```
### Its frequency :
```{r echo=FALSE}
cat("The frequency of the most frequent word is equal to :", max(No.7))
```
### the practical consequences :
TODO

## The appearance of aaaand :
```{r}
aaaand <- word_count_train[,6]
sum(aaaand)
```
*Comment:*
aaaand occurs just one time, it provides no information that can be used for this classification. May be it can be seen as a **Stop Word**, that is why we should remove it from the text before training our model.

# Question 3:

## Laplace smoothing
When applying Multinomial Naive Bayes to this classification problem, the frequency based probability might introduce zeros when multiplying probabilities, this will lead to a failure in preserving the information contributed by non-zero probabilities.
therefore, Laplace smoothing approach is adopted to counter this problem.

## Train the multinomial naive Bayes on the training set in the word count representation. Set the Laplacian parameter to 1.
```{r echo=FALSE}
#install.packages("naivebayes")
#install.packages("caret")
#install.packages("e1071")
library(naivebayes)
```

```{r}
x_train <- as.matrix(word_count_train[, -1])
y_train <- factor(word_count_train[, 1])
x_test <- as.matrix(word_count_test[, -1])
y_test <- factor(word_count_test[, 1])
### Train the Multinomial Naive Bayes
mnb <- multinomial_naive_bayes(x = x, y = y, laplace = 1)
summary(mnb)
```
```{r echo=FALSE}
library(ggplot2)
library(dplyr)

plot_cm<-function(cm){
  table <- data.frame(cm$table)
  
  plotTable <- table %>%
    mutate(goodbad = ifelse(table$Prediction == table$Reference, "good", "bad")) %>%
    group_by(Reference) %>%
    mutate(prop = Freq/sum(Freq))
  
  ggplot(data = plotTable, mapping = aes(x = Reference, y = Prediction, fill = goodbad, alpha = prop)) +
    geom_tile() +
    geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
    scale_fill_manual(values = c(good = "green", bad = "red")) +
    theme_bw() +
    xlim(rev(levels(table$Reference)))
}
```
## Predict labels both for training and test observations.
```{r}
# Classification :
y_train_hat <- predict(mnb, newdata = x_train, type = "class")
y_test_hat <- predict(mnb, newdata = x_test, type = "class")
# Confusion matrices
cm1 <- caret::confusionMatrix(y_train_hat, y_train)
cm2 <- caret::confusionMatrix(y_test_hat, y_test)
```
### Visualization of both training and test confusion matrices:
```{r}
plot_cm(cm1)
plot_cm(cm2)
```
### The misclassification rate:
```{r}
cat("misclassification error on the training set is :  ", 1-sum(diag(cm1$table))/sum(cm1$table))
cat("\nmisclassification error on the test set is     :  ", 1-sum(diag(cm2$table))/sum(cm2$table))
```
**Comment: **
This gives an intuition that the model has memorized the training data very well.
Moreover, it is not guaranteed to work on unseen data since the error on the test set is higher than the error on the training set.
We can say that we notice an indication of overfitting.

# Quesion 4:

## How many positive and negative reviews are there in the training set? In the test set?
```{r}
# training set
positive_train <- sum(tfidf_train$y)
negative_train <- length(tfidf_train$y) - positive_train
# test set
positive_test <- sum(tfidf_test$y)
negative_test <- length(tfidf_test$y) - positive_test
```

### Visualization of results
```{r echo=FALSE}
tf <- c(positive_train, positive_test, negative_train, negative_test)
tf_mx <- matrix(tf, nrow=2, byrow=TRUE)
names <- c("Training set", "Test set")
col <- c("green", "red")
ylim <- c(0, 1.1*1000)
ax <- barplot(tf_mx, main="Reviews of each dataset",col=col, names=names, ylim=ylim)
legend("topright", c("Positve reviews", "Negative reviews"), fill = col)
H<-apply((tf_mx), 2L, cumsum) - (tf_mx)/2
text(rep(ax, each=nrow(H)), H, labels = (tf_mx))
```
## Look at the tf-idf representation of the training set. What are the three most important words for negative reviews? 
```{r}
# negative reviews
tf_train_neg <- tfidf_train[which(tfidf_train$y==0),]
# sum of each column
neg_sum <- colSums(tf_train_neg)
# sort the data
n <- length(tf_train_neg)
sorted_neg <- order(neg_sum)[(n-2):n]
negativenames(tf_train_neg)[sorted_neg]
```
**Comment:** TODO


