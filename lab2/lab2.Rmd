---
title: "SADM_Lab2"
author: "El Ansary Otmane, Benchekroun Omar, Bensrhier Nabil"
date: "23/03/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PCA Regression in Genetics 

## 1 - DATA

The data is described by a map which locates the position of each population by a specific color symbol. We obtain the map below:

```{r echo=FALSE}
NAm2 = read.table("NAm2.txt", header=TRUE)
```

```{r}
names=unique(NAm2$Pop)
npop=length(names)
coord=unique(NAm2[,c("Pop","long","lat")]) #coordinates for each pop
colPalette=rep(c("black","red","cyan","orange","brown","blue","pink","purple","darkgreen"),3)
pch=rep(c(16,15,25),each=9)
plot(coord[,c("long","lat")],pch=pch,col=colPalette,asp=1)
# asp allows to have the correct ratio between axis longitude and latitude
# Thus the map is not deformed
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75, ncol=2,lwd=2)
library(maps);map("world",add=T)
```



This code simply plots the geographical positions of the countries found in the given dataset. The map of the world is also plotted behind so that we can see where the said countries are located.
***NB:*** The map obtained is similar to the map given in the statement.




## 2 - Regression
```{r echo=FALSE}
sink("error.txt", type=c("output", "message"))
NAaux = NAm2[,c(8:20)]
df = data.frame(NAaux)
reg <- lm(df$long~., df[, -1])
summary(reg)
# We should close all open sinks with
while (sink.number()>0) sink()
```

**Results:**
If we try to do the regression using all 5709 the markers, we get NA on the values like standard deviation et p-value, which means the regression was not computed successfully.

If we choose a subsample of only the first 20 markers to try and predict the longitude, we get mostly high p-values.

We conclude that there is not enough data to perform a multiple linear regression predictive model.


## 3 - PCA

### a) The principle of PCA:

Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.

This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set.

### b) 

```{r}
# Only genetic data
NAaux = NAm2[,-c(1:8)]
pcaNAm2 = prcomp(NAaux)
```

There is no need to scale the data, since the values are 0 and 1.
That means that genetic markers are binary encoded. So they don't really have a weight.


### c)

```{r}
caxes=c(1,2)
plot(pcaNAm2$x[,caxes],col="white")
for (i in 1:npop) {
#print(names[i])
lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],type="p",col=colPalette[i],pch=pch[i])
}
legend("top",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=3,lwd=2)
```

The populations easily identified using the first two PCs are the ones shown to be isolated in the plot, in this case : ***Ache*** and ***Surui***. However, all the other populations are condensed to the point that they can no longer be identified

```{r eval}
caxes=c(5,6)
plot(pcaNAm2$x[,caxes],col="white")
for (i in 1:npop) {
#print(names[i])
lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],type="p",
col=colPalette[i],pch=pch[i])
}
legend("top",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=3,lwd=2)
```

Using the 5th and 6th PCs, we can easily identifiy the Karitiana and Pima populations.

*Remark :* We need more than two axes to be able to identify the entire population.

### d)

```{r echo=FALSE}
sm = summary(pcaNAm2)
sm$importance[,1]
sm$importance[,2]
```
- Percentage of variance captured by the first 2 principal components is given by:
$$
\frac{\lambda_1 +\lambda_2}{\sum_{k=1}^{n} \lambda_k}
$$
 
```{r echo=FALSE}
cat("The first two components capture ", sm$importance[3,2] * 100, "%  of the total variance.")
```


**The minimal number of principal components **\\
- ***Method 1 :*** we calculate the cardinal of this set :
$$
\left\{\lambda , \lambda > \frac{1}{n}\sum_{k = 1}^{n} \lambda_k \right\} 
$$
```{r}

# set of eigenvalues
eigenvalues <- sm$importance[1, ] ^ 2
# number of these values
len = length(eigenvalues)
# mean of eigenvalues
mean <- mean(eigenvalues)
cardinal <- 0

for (i in 1:len)
{
  if (eigenvalues[i] > mean)
  {
    cardinal <- cardinal + 1
  }
  else
  {
    break()
  }
}
cat("We need the first ", cardinal, "PC's")
```
- ***Method 2 :*** To conserve around 90% of the total inertia: 

```{r echo=FALSE}
count <- 1
while (sm$importance[3,count] <= 0.9)
{
  count <- count + 1
}
cat("We need the first ", count, "PC's")
```

## 4) PCR (Principal Components Regression):

### a)

```{r}
pcs <- pcaNAm2$x[,1:250]
lmlong <- lm(NAm2$long~pcs)
lmlat <- lm(NAm2$lat~pcs)
```

```{r}
plot(lmlong$fitted.values,lmlat$fitted.values,col="white", asp = 1)
for (i in 1:npop) {
#print(names[i])
lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],
lmlat$fitted.values[which(NAm2[,3]==names[i])],type="p",
col=colPalette[i],pch=pch[i])
}
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,
cex=.75,ncol=3,lwd=2)
map("world",add=T)
```


Compared to the map of question 1, we notice a similarity of the geographical positions with an additional important noise, which is completely predictable since we're only using the first 250 Principal Components. 

We would say that it illustrates optimistically the ability to find geographical origin of
individuals outside the database from its genetic markers.


### b)
```{r warning=FALSE}
library("fields")
```

```{r}
matrix = rdist.earth(cbind(lmlong$fitted.values,lmlat$fitted.values),
            cbind(NAm2$long, NAm2$lat), miles=FALSE)
error = mean(diag(matrix))
cat("The mean error of the previous model built is : ", error, " \n")
```

## 5) PCR and cross-validation:

### a)

Cross Validation is a technique used to determine the quality of the built model, the steps for a k fold cross validation are:

1 - Shuffle the dataset randomly.

2 - Split the dataset into k groups

3 - For each unique group:

   - Take the group as a hold out or test data set
      
   - Take the remaining groups as a training data set
      
   - Fit a model on the training set and evaluate it on the test set
      
   - Retain the evaluation score and discard the model
      
4 - Summarize the skill of the model using the sample of model evaluation scores



```{r}
k = 10
labels=rep(1:k,each=(dim(NAm2)[1]/k))
labels=c(labels, 1:(dim(NAm2)[1]%%k))
set=sample(labels,dim(NAm2)[1])
```



### b)
  

```{r}
nindiv <- dim(NAm2)[1]

pcapredict <- function(naxes){
  predictedCoord <- matrix(nrow = dim(df)[1], ncol = 2)
  colnames(predictedCoord) = c("longitude", "latitude")
  
  for (f in 1:k){
    # long
    pcalong.test=data.frame(cbind(long=NAm2[(set==f),c("long")],pcaNAm2$x[set==f, 1:(1+naxes)]))
    pcalong.train = data.frame(cbind(long=NAm2[(set!=f),c("long")],pcaNAm2$x[set!=f, 1:(1+naxes)]))
    reg <- lm(pcalong.train$long~., pcalong.train) 
    predlong = predict(reg, pcalong.test)
    
    #lat
    pcalat.test=data.frame(cbind(lat=NAm2[(set==f),c("lat")],pcaNAm2$x[set==f, 1:(1+naxes)]))
    pcalat.train = data.frame(cbind(lat=NAm2[(set!=f),c("lat")],pcaNAm2$x[set!=f, 1:(1+naxes)]))
    reg <- lm(pcalat.train$lat~., pcalat.train) 
    predlat = predict(reg, pcalat.test)
    
    predictedCoord[set==f] = cbind(predlong, predlat)
  }
  matrix = rdist.earth(cbind(predictedCoord[,1],predictedCoord[,2]),
              cbind(NAm2[,c("long")], NAm2[,c("lat")]), miles=FALSE)
  error = mean(diag(matrix))
  return (error)
}

cat("The prediction error is: ", pcapredict(4))
```


### c)

```{r echo=FALSE, warning=FALSE,message=FALSE}
s = seq(2,440,by=10)
p =  sapply(s, pcapredict)
```

We repeat the same steps.
We then draw the graph of prediction errors .


```{r}
plot(s,p, col="red" ,xlab="nAxes", ylab="Predicted error")
```

### d)

We should keep the model that minimizes the prediction error: 

```{r echo=FALSE}
nbmin = s[p==min(p)]
cat("That would be the one with the first",nbmin, "principal components.")
```




```{r echo=FALSE}
min = min(p)
cat("Its prediction error is", min)
```


Computing the training error is tricky, since the matrix predictedCoord is not big enough to fill with with 10 times the training set (redunduncies in the training sets). We choose to compute the matrix and its error for each fold, and then compute the mean of these errors.


```{r}
naxes = nbmin
errors = c()
for (f in 1:k){
  predictedCoord <- cbind(rep(0,nindiv),rep(0,nindiv)) #initialization for each fold
  
  pcalong.test=data.frame(cbind(long=NAm2[(set==f),c("long")],pcaNAm2$x[set==f, 1:(1+naxes)]))
  pcalong.train = data.frame(cbind(long=NAm2[(set!=f),c("long")],pcaNAm2$x[set!=f, 1:(1+naxes)]))
  reg <- lm(pcalong.train$long~., pcalong.train)
  predlong = predict(reg, pcalong.train)

  pcalat.test=data.frame(cbind(lat=NAm2[(set==f),c("lat")],pcaNAm2$x[set==f, 1:(1+naxes)]))
  pcalat.train = data.frame(cbind(lat=NAm2[(set!=f),c("lat")],pcaNAm2$x[set!=f, 1:(1+naxes)]))
  reg <- lm(pcalat.train$lat~., pcalat.train)
  predlat = predict(reg, pcalat.train)
  
  predictedCoord[set!=f] = cbind(predlong, predlat)
  matrix = rdist.earth(cbind(predictedCoord[,1],predictedCoord[,2]),
            cbind(NAm2[,c("long")], NAm2[,c("lat")]), miles=FALSE)
  error = mean(diag(matrix))
  errors = c(errors, error)
}
cat("The training error is", mean(errors))
```


The training error is naturally smaller than the test error, the model always fits the training data better. However, the difference is not big which means we don't have an overfitting problem, which is the whole reason why we use methods such as cross validation.




## 6) Conclusion
 
This study demonstrates the usefulness of Principal Component Analysis in understanding the data and its predictive power.

The data given presentes an interesting feature, the number of individuals is much smaller than that of the predictors, which makes a multiple linear regression impossible to achieve.

PCA gives us new variables (components) that are a linear combinations of the initial variables (genetic markers), theses variables are orthogonal and ranked by their corresponding variance (from the data).

The first results were promising. Projecting the data one the first two components gives us the ability to distinguish certain populations from the rest. Obviously, it's not enough and the usage of further more components would prove to be more complete.

A preliminary idea would be to use the first 350 principal components to conserve 90% of the total inertia and perform a linear regression on the PCs.
However, performing regression on these PC's may cause an overfitting problem (too many). Therefore, it is necessary to perform the 10 fold cross validation method to determine the best number of principal components to use in the prediction, without relying too much on the data itself (changing the validation set 10 times).

As expected, the optimal number of principal components (=72) is much less than 350, but it is the most accurate that we can achieve given the data, while escaping the trap of overfitting (hyperparametrization).

Further improvements would be to use artificial neural nets, and experiment with different layers and optimizers, we would need of course to perform a 10-fold cross validation method on that model as well.




