---
title: "lab2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PCA Regression in Genetics 

## 1 - 

```{r}
NAm2 = read.table("NAm2.txt", header=TRUE)
```

```{r}
names=unique(NAm2$Pop)
npop=length(names)
coord=unique(NAm2[,c("Pop","long","lat")]) #coordinates for each pop
colPalette=rep(c("black","red","cyan","orange","brown","blue","pink","purple","darkgreen"),3)
pch=rep(c(16,15,25),each=9)
plot(coord[,c("long","lat")],pch=pch,col=colPalette,asp=1)
# asp allows to have the correct ratio between axis longitude and latitude
# Thus the map is not deformed
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75, ncol=2,lwd=2)
library(maps);map("world",add=T)
```


This code simply plots the geographical positions of the populations found in the given dataset. The map of the world is also plotted behind so that we can see where the populations are located.



## 2 - Regression
```{r}
NAaux = NAm2[,c(8:20)]
df = data.frame(NAaux)
reg <- lm(df$long~., df)
summary(reg)
```


If we try to do the regression using all 5709 the markers, we get NA on the values like standard deviation et p-value, which means the regression was not computed successfully.

If we choose a subsample of only the first 20 markers to try and predict the longitude, we get mostly high p-values.

We conclude that there is not enough data to perform a multiple linear regression predictive model.




## 3 - PCA

### a)

Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.

This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set.

### b)

```{r}
NAaux = NAm2[,-c(1:8)]
pcaNAm2 = prcomp(NAaux)
```

There is no need to scale the data, since the values are 0 and 1.



### c)

```{r}
caxes=c(1,2)
plot(pcaNAm2$x[,caxes],col="white")
for (i in 1:npop) {
#print(names[i])
lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],type="p",col=colPalette[i],pch=pch[i])
}
legend("top",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=3,lwd=2)
```

The populations easily identified using the first two PCs are the ones shown to be isolated in the plot, in this case : Ache and Surui.

```{r eval}
caxes=c(5,6)
plot(pcaNAm2$x[,caxes],col="white")
for (i in 1:npop) {
#print(names[i])
lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],type="p",
col=colPalette[i],pch=pch[i])
}
legend("top",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=3,lwd=2)
```

Using the 5th and 6th PCs, we can easily identifiy the Karitiana and Pima populations.


### d)

```{r echo=FALSE}
sm = summary(pcaNAm2)
sm$importance[,1]
sm$importance[,2]
```

The first two components capture 
```{r echo=FALSE}
sm$importance[3,2]
```
of the total variance.



To conserve around 90% of the total inertia, we need the first 350 PCs.
```{r echo=FALSE}
sm$importance[,350]
```



## 4)

### a)

```{r}
pcs <- pcaNAm2$x[,1:250]
lmlong <- lm(NAm2$long~pcs)
lmlat <- lm(NAm2$lat~pcs)
```

```{r}
plot(lmlong$fitted.values,lmlat$fitted.values,col="white", asp = 1)
for (i in 1:npop) {
#print(names[i])
lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],
lmlat$fitted.values[which(NAm2[,3]==names[i])],type="p",
col=colPalette[i],pch=pch[i])
}
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,
cex=.75,ncol=3,lwd=2)
map("world",add=T)
```


Compared to the map of question 1, we notice a similarity of the geographical positions with an additional important noise, which is completely predictable since we're only using the first 250 Principal Components. 

We would say that it illustrates optimistically the ability to find geographical origin of
individuals outside the database from its genetic markers.


### b)
```{r echo = FALSE}
library("fields")
```

```{r}
matrix = rdist.earth(cbind(lmlong$fitted.values,lmlat$fitted.values),
            cbind(NAm2[,7], NAm2[,8]), miles=FALSE)
error = mean(diag(matrix))
error
```


## 5)

### a)

Cross Validation is a technique used to determine the quality of the built model, the steps for a k fold cross validation are:

1 - Shuffle the dataset randomly.

2 - Split the dataset into k groups

3 - For each unique group:

   - Take the group as a hold out or test data set
      
   - Take the remaining groups as a training data set
      
   - Fit a model on the training set and evaluate it on the test set
      
   - Retain the evaluation score and discard the model
      
4 - Summarize the skill of the model using the sample of model evaluation scores



```{r}
k = 10
labels=rep(1:k,each=(dim(NAm2)[1]/k))
labels=c(labels, 1:(dim(NAm2)[1]%%k)  )
set=sample(labels,dim(NAm2)[1])
```



### b)


```{r}
nindiv <- dim(NAm2)[1]

pcapredict <- function(naxes){
  predictedCoord <- cbind(rep(0,nindiv),rep(0,nindiv))
  #pcalong=data.frame(cbind(long=NAm2[,c("long")],pcaNAm2$x))
  for (f in 1:k){
    pcalong.test=data.frame(cbind(long=NAm2[(set==f),c("long")],pcaNAm2$x[set==f, 1:(1+naxes)]))
    pcalong.train = data.frame(cbind(long=NAm2[(set!=f),c("long")],pcaNAm2$x[set!=f, 1:(1+naxes)]))
    reg <- lm(pcalong.train$long~., pcalong.train)
    predlong = predict(reg, pcalong.test)
    
    pcalat.test=data.frame(cbind(lat=NAm2[(set==f),c("lat")],pcaNAm2$x[set==f, 1:(1+naxes)]))
    pcalat.train = data.frame(cbind(lat=NAm2[(set!=f),c("lat")],pcaNAm2$x[set!=f, 1:(1+naxes)]))
    reg <- lm(pcalat.train$lat~., pcalat.train)
    predlat = predict(reg, pcalat.test)
    
    predictedCoord[set==f] = cbind(predlong, predlat)
  }
  matrix = rdist.earth(cbind(predictedCoord[,1],predictedCoord[,2]),
              cbind(NAm2[,c("long")], NAm2[,c("lat")]), miles=FALSE)
  error = mean(diag(matrix))
  return (error)
}

pcapredict(4)
```


### c)

```{r}
s = seq(2,440,by=10)
p =  sapply(s, pcapredict)
```




```{r}
plot(s,p)
```

### d)

We should keep the model that minimizes the prediction error, that would be the one with the first

```{r echo=FALSE}
nbmin = s[p==min(p)]
nbmin
```

principal components.

Its prediction error is
```{r echo=FALSE}
min = min(p)
min
```

We choose to compute the training error by computing the mean of the errors in each training set. We can't put everything in a matrix like the test data because we will have redunduncies.


Computing the training error is tricky, since the matrix predictedCoord is not big enough to fill with with 10 times the training set (redunduncies in the training sets). We choose to compute the matrix and its error for each fold, and then compute the mean of these errors.

The training error is therefore :

```{r}
naxes = nbmin
errors = c()
for (f in 1:k){
  predictedCoord <- cbind(rep(0,nindiv),rep(0,nindiv)) #initialization for each fold
  
  pcalong.test=data.frame(cbind(long=NAm2[(set==f),c("long")],pcaNAm2$x[set==f, 1:(1+naxes)]))
  pcalong.train = data.frame(cbind(long=NAm2[(set!=f),c("long")],pcaNAm2$x[set!=f, 1:(1+naxes)]))
  reg <- lm(pcalong.train$long~., pcalong.train)
  predlong = predict(reg, pcalong.train)

  pcalat.test=data.frame(cbind(lat=NAm2[(set==f),c("lat")],pcaNAm2$x[set==f, 1:(1+naxes)]))
  pcalat.train = data.frame(cbind(lat=NAm2[(set!=f),c("lat")],pcaNAm2$x[set!=f, 1:(1+naxes)]))
  reg <- lm(pcalat.train$lat~., pcalat.train)
  predlat = predict(reg, pcalat.train)
  
  predictedCoord[set!=f] = cbind(predlong, predlat)
  matrix = rdist.earth(cbind(predictedCoord[,1],predictedCoord[,2]),
            cbind(NAm2[,c("long")], NAm2[,c("lat")]), miles=FALSE)
  error = mean(diag(matrix))
  errors = c(errors, error)
}
mean(errors)
```


The training error is naturally smaller than the test error, the model always fits the training data better. However, the difference is not big which means we don't have an overfitting problem, which is the whole reason why we use methods such as cross validation.




## 6)
 
This study demonstrates the usefulness of Principal Component Analysis in understanding the data and its predictive power.

The data given presentes an interesting feature, the number of individuals is much smaller than that of the predictors, which makes a multiple linear regression impossible to achieve.

PCA gives us new variables (components) that are a linear combinations of the initial variables (genetic markers), theses variables are orthogonal and ranked by their corresponding variance (from the data).

The first results were promising. Projecting the data one the first two components gives us the ability to distinguish certain populations from the rest. Obviously, it's not enough and the usage of further more components would prove to be more complete.

A preliminary idea would be to use the first 350 principal components to conserve 90% of the total inertia and perform a linear regression on the PCs.
However, performing regression on these PC's may cause an overfitting problem (too many). Therefore, it is necessary to perform the 10 fold cross validation method to determine the best number of principal components to use in the prediction, without relying too much on the data itself (changing the validation set 10 times).

As expected, the optimal number of principal components (=72) is much less than 350, but it is the most accurate that we can achieve given the data, while escaping the trap of overfitting (hyperparametrization).

Further improvements would be to use artificial neural nets, and experiment with different layers and optimizers, we would need of course to perform a 10-fold cross validation method on that model as well.




